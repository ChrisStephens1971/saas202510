# pytest-benchmark configuration
[benchmark]
# Only run benchmarks when explicitly requested
disable = False

# Save benchmark results
autosave = True
save-data = True

# Minimum rounds for benchmarks
min-rounds = 5

# Calibration precision
calibration-precision = 10

# Timer resolution
timer = time.perf_counter

# Storage
storage = file:.benchmarks
json = True

# Comparison
compare = 0001
compare-fail = mean:5%

# Display
verbose = True
columns = min, max, mean, stddev, median, iqr, outliers, rounds, iterations

# Histogram
histogram = normal
